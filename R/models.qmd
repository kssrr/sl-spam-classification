---
title: "Building neural network & competitor"
editor: source
date: today
format: 
  html:
    embed-resources: true
    message: false
    warning: false
    highlight-style: pygments
editor_options: 
  chunk_output_type: console
---

<style>
@import url('https://fonts.googleapis.com/css2?family=Inconsolata:wght@200..900&display=swap');

pre, code {
  font-family: 'Inconsolata', monospace;
  font-size: 18px !important;
}
</style>

# Packages

```{r}
library(discrim)
library(keras3)
library(tensorflow)
library(tidymodels)
```

# Data

```{r}
set.seed(42)

spam <- readr::read_csv(here::here("data/spam.csv"))

spam <- 
  spam |> 
  mutate(
    spam = factor(
      if_else(spam == 0, "no spam", "spam"),
      ordered = TRUE,
      levels = c("spam", "no spam")
    )
  )

spam_split <- initial_validation_split(spam, prop = c(0.6, 0.2), strata = "spam")

train <- training(spam_split)
val <- validation(spam_split)
test <- testing(spam_split)
```

Getting an overview:

```{r}
glimpse(train)
```

A lot of these seem to be word or character frequencies, so they will likely have very skewed distributions.

# Preprocessing

Fitting the recipe to the training data (to avoid leakage). Synthetic minority class oversampling, to get the same amount of examples for both classes, log-transforming[^log] Z-standardizing, dropping highly correlated features & those with near-zero variance:

[^log]: Most of these features are word counts of some sort, likely with very skewed distributions.

```{r}
spam_rec <- 
  recipe(spam ~ ., data = train) |> 
  themis::step_smote(spam, over_ratio = 1, neighbors = 5) |> 
  step_log(all_numeric_predictors(), offset = 1) |> 
  step_range(all_numeric_predictors(), min = 0, max = 1) |> 
  step_corr(all_numeric_predictors(), threshold = 0.9) |> 
  step_nzv(all_numeric_predictors())
```

# EDA

Class distribution:

```{r}
train |> 
  count(spam) |> 
  ggplot(aes(x = spam, y = n, color = spam, fill = spam)) +
  geom_col(alpha = 0.7) +
  theme_minimal() +
  labs(
    title = "Class Distribution",
    x = "", 
    y = "N. of obs"
  ) +
  theme(legend.position = "none")
```

Feature correlations:

```{r}
corrs <- 
  train |> 
  select(-spam) |> 
  cor() |> 
  as.data.frame() |> 
  rownames_to_column(var = "x1") |> 
  tibble() |> 
  pivot_longer(-x1, names_to = "x2", values_to = "val")

corrs |> 
  ggplot(aes(x = x1, y = x2, fill = val)) +
  geom_tile() +
  scale_fill_distiller(palette = "RdYlGn", direction = 1, limits = c(-1, 1)) +
  theme(axis.text = element_blank(), axis.ticks = element_blank()) +
  labs(
    title = "Pairwise correlations", 
    subtitle = "All features", 
    fill = "Pearson's r", 
    x = "",
    y = ""
  )
```

PCA: checking if we can find some patterns in a transformed representation (also seeing if PCA as part of the preprocessing pipeline might make sense)

```{r}
spam_rec |> 
  step_pca(all_numeric_predictors(), num_comp = 4) |> 
  prep() |> 
  bake(new_data = train) |> 
  ggplot(aes(x = .panel_x, y = .panel_y, color = spam, fill = spam)) +
  geom_point(alpha = 0.4, size = 0.5) +
  ggforce::geom_autodensity(alpha = .3) +
  ggforce::facet_matrix(vars(-spam), layer.diag = 2) + 
  scale_color_brewer(palette = "Dark2", direction = -1) + 
  scale_fill_brewer(palette = "Dark2", direction = -1) +
  theme_minimal() +
  labs(title = "Principal Component Analysis", fill = "", color = "")
```

# Neural Network Classifier

Preparing the data (separate features & labels, bring into matrix format). We also need to apply the fitted preprocessing pipeline here for the data going into keras. `prep()` fits the recipe (on the training data, we specified this in the recipe), and `bake()` applies the transformation (equivalent to `.fit()` and `.transform()` in sklearn pipelines):

```{r}
keras_split <- function(set) {
  df <- 
    set |> 
    mutate(spam = if_else(spam == "spam", 1, 0))
  
  list(
    X = df |> select(-spam) |> as.matrix() |> unname(),
    y = df |> pull(spam) |> as.matrix()
  )
}

keras_train <- spam_rec |> prep() |> bake(new_data = train) |> keras_split()
keras_val <-  spam_rec |> prep() |> bake(new_data = val) |> keras_split()
keras_test <- spam_rec |> prep() |> bake(new_data = test) |> keras_split()

X_train <- keras_train$X
y_train <- keras_train$y
X_val <- keras_val$X
y_val <- keras_val$y
X_test <- keras_test$X
y_test <- keras_test$y
```

**Model**:

```{r}
keras3::set_random_seed(42) 
#^ the keras3 version (supposedly) sets a seed for the R session and the whole backend

mlp <- keras_model_sequential(
  layers = list(
    layer_dense(units = 128, activation = "relu", kernel_regularizer = regularizer_l2(0.001)),
    layer_dropout(rate = 0.25),
    layer_dense(units = 64, activation = "relu", kernel_regularizer = regularizer_l2(0.001)),
    layer_dropout(rate = 0.25),
    layer_dense(units = 32, activation = "relu", kernel_regularizer = regularizer_l2(0.001)),
    layer_dropout(rate = 0.25),
    layer_dense(units = 16, activation = "relu", kernel_regularizer = regularizer_l2(0.001)),
    layer_dropout(rate = 0.25),
    layer_dense(units = 1, activation = "sigmoid")
  )
)
```

Compiling:

```{r}
keras3::set_random_seed(42)
# chunks are independent when rendering apparently, so again...

mlp |> 
  compile(
    optimizer = optimizer_adam(learning_rate = 0.001),
    loss = "binary_crossentropy",
    metrics = list(
      metric_binary_accuracy(),
      metric_precision(),
      metric_recall(),
      metric_auc()
    )
  )
```

**Training**:

```{r}
keras3::set_random_seed(42)

history <- 
  mlp |> 
  fit(
    x = X_train,
    y = y_train,
    epochs = 250L,
    batch_size = 32L,
    validation_data = list(X_val, y_val),
    callbacks = list(
      # early stopping:
      callback_early_stopping(
        monitor = "val_loss",
        patience = 5L,
        restore_best_weights = TRUE 
      ),
      # schedule learning rate:
      callback_reduce_lr_on_plateau(
        monitor = "val_loss",
        factor = 0.8,
        patience = 3L,
        min_lr = 0.00001
      )
    ),
    shuffle = FALSE
  )
```

Details on the trained model:

```{r}
summary(mlp)
```

Visually:

```{r}
mlp |> plot(show_shapes = TRUE, show_trainable = TRUE)
```

Looking at the training progression:

```{r}
training_prog <- 
  history |> 
  as.data.frame() |> 
  tibble() |>
  pivot_wider(values_from = "value", names_from = "metric") |> 
  drop_na(loss)
```

Loss curves:

```{r}
training_prog |> 
  ggplot(aes(x = epoch, y = loss, color = data)) +
  geom_line() +
  theme_minimal() +
  labs(
    title = "Training curves",
    subtitle = "Binary cross-entropy loss on training and validation sets, over epochs",
    x = "Epochs",
    y = "Loss",
    color = "Data"
  )
```

Validation metrics:

```{r}
training_prog |> 
  select(-c(learning_rate, loss)) |> 
  pivot_longer(-c(epoch, data), names_to = "metric", values_to = "value") |> 
  ggplot(aes(x = epoch, y = value, color = data)) +
  geom_line() +
  facet_wrap(~metric) +
  theme_minimal() +
  labs(
    title = "Training improvements",
    subtitle = "Development of metrics over epochs, validation set",
    x = "Epochs",
    y = "",
    color = "Data"
  )
```

Collecting final metrics for training set:

```{r}
class_metrics <- metric_set(accuracy, precision, f_meas)

mlp_metrics_train <- 
  mlp$predict(X_train) |> 
  round() |> 
  as.vector() |> 
  tibble(mlp_pred = _) |> 
  bind_cols(train) |> 
  mutate(mlp_pred = factor(if_else(mlp_pred == 1, "spam", "no spam"), levels = c("spam", "no spam"))) |> 
  class_metrics(truth = spam, estimate = mlp_pred) |> 
  select(-.estimator) |> 
  pivot_wider(names_from = ".metric", values_from = ".estimate") |> 
  mutate(name = "Neural Network")
```


Evaluating the model on the test set:

```{r}
mlp_preds <- 
  mlp$predict(X_test) |> 
  round() |> 
  as.vector() |> 
  tibble(mlp_pred = _) |> 
  bind_cols(test) |> 
  mutate(mlp_pred = factor(if_else(mlp_pred == 1, "spam", "no spam"), levels = c("spam", "no spam")))
```

Metrics:

```{r}
mlp_preds |> 
  class_metrics(truth = spam, estimate = mlp_pred)
```

Confusion matrix:

```{r}
mlp_preds |> 
  conf_mat(truth = spam, estimate = mlp_pred) |> 
  autoplot(type = "heatmap")
```

# Competitors

10-fold cross validation:

```{r}
set.seed(42)

folds <- vfold_cv(train, v = 10, strata = "spam")
```

To select a proper competitor, I tune three "traditional" models:

* Penalized logistic regression
* Random Forest
* Naive Bayes

```{r}
set.seed(42)

log_spec <- logistic_reg(
  mode = "classification",
  engine = "glmnet",
  penalty = tune(),
  mixture = 1 # pure L1 regularization
)

log_grid <- tibble(penalty = 10^seq(-5, -1, length.out = 50))

rf_spec <- rand_forest(
  mode = "classification",
  mtry = tune(),
  trees = tune(),
  min_n = tune()
) |> set_engine("ranger", importance = "impurity") # variable importance

rf_grid <- expand_grid(
  mtry = c(4, 8, 12),
  trees = c(100, 500, 1000),
  min_n = c(5, 10, 20)
)

# For naive bayes, we disable kernel density estimation. This yields
# Gaussian naive bayes, so we assume normally distributed features
nb_spec <- naive_Bayes(
  mode = "classification",
  smoothness = tune()
) |> set_engine("naivebayes", usekernel = FALSE)

# just using raw probabilities (smoothing has no effect for Gaussian bayes
# with only numerical features I believe), but also cross-validating
nb_grid <- tibble(smoothness = 0)
```

Glueing together & tuning:

```{r}
set.seed(42)

models <- tribble(
  ~name,                 ~spec,    ~grid,
  "Logistic Regression", log_spec, log_grid,
  "Random Forest",       rf_spec,  rf_grid,
  "Naive Bayes",         nb_spec,  nb_grid
)

models <- 
  models |> 
  mutate(
    workflow = map(spec, \(m) workflow() |> add_recipe(spam_rec) |> add_model(m)),
    tuning_res = map2(workflow, grid, function(wf, g) {
      tune_grid(
        wf, 
        resamples = folds,
        grid = g,
        metrics = metric_set(accuracy, precision, f_meas),
        control = control_grid(verbose = TRUE, save_pred = TRUE)
      )
    }),
    metrics = map(tuning_res, collect_metrics)
  )
```

## Tuning results

First, adding 95% confidence intervals to metric estimates (estimated using the results collected across folds):

```{r}
models <- 
  models |> 
  mutate(
    metrics = map(metrics, function(m) { 
      m |> mutate(lower = mean - 1.96 * std_err, upper = mean + 1.96 * std_err)
    })
  )
```

### Logistic Regression

Here, we tuned the `penalty` parameter:

```{r}
models |> 
  filter(name == "Logistic Regression") |> 
  select(name, metrics) |> 
  unnest(metrics) |> 
  ggplot(aes(x = penalty, y = mean, fill = .metric)) +
  geom_ribbon(aes(ymin = lower, ymax = upper), alpha = 0.2) +
  geom_point(aes(color = .metric)) +
  geom_line(aes(color = .metric)) +
  facet_wrap(~.metric, scales = "free_y") +
  scale_x_log10() +
  theme_minimal() +
  theme(legend.position = "none") +
  labs(
    title = "Logistic Regression", 
    subtitle = "Tuning Results", 
    x = expression(lambda),
    y = "",
    caption = "Shaded area indicates 95% confidence interval.\nLogarithmic X-axis."
  )
```

### Random Forest

```{r}
models |> 
  filter(name == "Random Forest") |> 
  select(name, metrics) |> 
  unnest(metrics) |> 
  mutate(min_n = factor(min_n, ordered = TRUE)) |> 
  ggplot(aes(x = mtry, y = mean, group = min_n)) +
  geom_ribbon(aes(ymin = lower, ymax = upper, fill = min_n), alpha = .1) +
  geom_point(aes(color = min_n, )) +
  geom_line(aes(color = min_n, )) +
  facet_grid(vars(.metric), vars(trees), scale = "free_y") +
  theme_minimal() +
  labs(
    title = "Random Forest",
    subtitle = "Tuning Results, by number of trees",
    x = "Number of randomly sampled features",
    y = "",
    fill = "Min. N in \nnode for split",
    color = "Min. N in \nnode for split",
    caption = "Shaded area indicates 95% confidence interval."
  )
```

### Naive Bayes

```{r}
models |> 
  filter(name == "Naive Bayes") |> 
  select(name, metrics) |> 
  unnest(metrics) |> 
  ggplot(aes(x = smoothness, y = mean, color = .metric)) +
  geom_point() +
  geom_linerange(aes(ymin = lower, ymax = upper)) +
  facet_wrap(~.metric, nrow = 1) +
  theme_minimal() +
  theme(legend.position = "none", axis.text.x = element_blank()) +
  labs(
    title = "Naive Bayes",
    subtitle = "Metric estimates across 10 folds, with 95% confidence interval",
    x = "",
    y = ""
  )
```

## Uncertainty estimation for best competitors

After tuning, selecting the best models by (1) accuracy, and (2) precision, and showing their uncertainty across folds:

```{r}
best_train <- 
  models |> 
  mutate(
    # best models by accuracy & precision:
    best_acc = map(tuning_res, \(res) select_best(res, metric = "accuracy")),
    best_prec = map(tuning_res, \(res) select_best(res, metric = "precision")),
    # metrics only for best models:
    across(c(best_acc, best_prec), function(params) {
      map2(tuning_res, params, function(res, p) {
        res |> 
          collect_metrics() |> 
          filter(.config == p$.config)
      })
    }, .names = "{col}_train"),
    # add 95% confidence interval:
    across(ends_with("_train"), function(metrics) {
      map(metrics, function(d) {
        d |> 
          mutate(lower = mean - 1.96 * std_err, upper = mean + 1.96 * std_err)
      })
    })
  ) |> 
  select(name, starts_with("best"))
```

Accuracy:

```{r}
best_train |> 
  select(name, best_acc_train) |> 
  unnest(best_acc_train) |> 
  filter(.metric == "accuracy") |> 
  ggplot(aes(x = name, y = mean, color = name)) +
  geom_hline(yintercept = mlp_metrics_train$accuracy, lty = "dashed", color = "grey") +
  geom_point() +
  geom_linerange(aes(ymin = lower, ymax = upper)) +
  theme_minimal() +
  labs(
    title = "Uncertainty of accuracy estimates",
    subtitle = "Competitor models, training data\nEstimated on 10 folds",
    x = "",
    y = "Accuracy",
    caption = "Bars indicate 95% confidence interval"
  ) +
  theme(legend.position = "none") +
  annotate(
    "text", 
    x = 0.67, 
    y = mlp_metrics_train$accuracy + 0.002, 
    label = "Neural Network",
    color = "grey50",
    size = 4
  )
```

Precision (classifying emails as spam, false positives - i.e. mistakenly labeling "ham" emails as spam - are more costly):

```{r}
best_train |> 
  select(name, best_prec_train) |> 
  unnest(best_prec_train) |> 
  filter(.metric == "precision") |> 
  ggplot(aes(x = name, y = mean, color = name)) +
  geom_hline(yintercept = mlp_metrics_train$accuracy, lty = "dashed", color = "grey") +
  geom_point() +
  geom_linerange(aes(ymin = lower, ymax = upper)) +
  theme_minimal() +
  labs(
    title = "Uncertainty of precision estimates",
    subtitle = "Competitor models, training data\nEstimated on 10 folds",
    x = "",
    y = "Precision",
    caption = "Bars indicate 95% confidence interval"
  ) +
  theme(legend.position = "none") +
  annotate(
    "text", 
    x = 0.67, 
    y = mlp_metrics_train$accuracy + 0.002, 
    label = "Neural Network",
    color = "grey50",
    size = 4
  )
```

# Random Forest vs. Neural Network on test set

```{r}
rf_res <- models |> filter(name == "Random Forest")
rf_tuning_res <- rf_res |> pull(tuning_res) |> pluck(1)
rf_wf <- rf_res |> pull(workflow) |> pluck(1)
```

Considering the best random forest model by precision (most important metric here):

```{r}
rf_fit_prec <- 
  rf_wf |> 
  finalize_workflow(select_best(rf_tuning_res, metric = "precision")) |>
  fit(train)
```

Test predictions for both (plus predicted class probability):

```{r}
nn_preds <- 
  mlp |> 
  predict(X_test) |> 
  as.vector() |> 
  tibble(.pred_spam = _) |> 
  mutate(
    .pred_no_spam = 1 - .pred_spam, 
    .pred_class = round(.pred_spam),
    .pred_class = factor(
      if_else(.pred_class == 1, "spam", "no spam"),
      ordered = TRUE,
      levels = c("spam", "no spam")
    ),
    model = "Neural Network"
  ) |> 
  bind_cols(test |> select(actual = spam))

rf_preds <- 
  rf_fit_prec |> 
  predict(test) |> 
  bind_cols(rf_fit_prec |> predict(test, type = "prob")) |>
  rename(.pred_no_spam = `.pred_no spam`) |> 
  mutate(
    model = "Random Forest",
    .pred_class = factor(.pred_class, ordered = TRUE, levels = c("spam", "no spam"))
  ) |> 
  bind_cols(test |> select(actual = spam))

test_preds <- bind_rows(nn_preds, rf_preds)
```

Metrics:

```{r}
# Some markdown magic:
mark_best <- function(x) {
  map_chr(x, function(val) {
    if (val == max(x))
      return(paste0("**", as.character(round(val, 3)), "**"))
    as.character(round(val, 3))
  })
}

test_preds |> 
  group_by(model) |> 
  nest(-model) |> 
  mutate(
    metrics = map(data, \(preds) {
      preds |> 
        class_metrics(truth = actual, estimate = .pred_class) |> 
        select(-.estimator) |> 
        pivot_wider(names_from = ".metric", values_from = ".estimate")
    })
  ) |> 
  select(model, metrics) |> 
  unnest(metrics) |> 
  rename(f1 = f_meas) |> 
  ungroup() |> 
  select(model, precision, everything()) |> 
  mutate(across(-model, mark_best)) |> 
  rename_with(stringr::str_to_title) |> 
  knitr::kable()
```

Looking at model confidence. Graphically, we can see that the neural network is more confident in its correct predictions, but also overconfident in its wrong predictions:

```{r}
confidence <- 
 test_preds |> 
  mutate(
    confidence = if_else(.pred_class == "spam", .pred_spam, .pred_no_spam),
    correct = if_else(actual == .pred_class, "correct", "incorrect")
  )

confidence |> 
  ggplot(aes(x = correct, y = confidence, fill = model, color = model)) +
  geom_hline(yintercept = c(.5, 1), lty = "dotted", color = "grey50") +
  geom_boxplot(position = position_dodge(width = 0.2), width = .1, outliers = FALSE, alpha = .5) +
  theme_minimal() +
  labs(
    title = "Confidence in predictions",
    subtitle = "By correct/incorrect classification",
    x = "",
    y = "Predicted class probability",
    fill = "Model",
    color = "Model"
  )
```

Other way of looking at it (making both & then deciding later for report):

```{r}
confidence |> 
  ggplot(aes(x = confidence, color = model, fill = model)) +
  geom_density(alpha = .34) +
  facet_wrap(~correct, nrow = 2, scale = "free_y") +
  theme_minimal()  +
  labs(
    title = "Confidence in predictions",
    subtitle = "By correct/incorrect classification",
    x = "Predicted class probability",
    y = "Density",
    fill = "Model",
    color = "Model"
  ) +
  theme(aspect.ratio = .5)
```


ROC curves:

```{r}
test_preds |> 
  group_by(model) |>
  roc_curve(truth = actual, .pred_spam) |>
  ungroup() |> 
  ggplot(aes(x = 1 - specificity, y = sensitivity, color = model)) +
  geom_line() + 
  geom_abline(linetype = "dotted", color = "grey50") +
  theme_minimal() +
  labs(
    title = "ROC Curves",
    subtitle = "Neural Network & Random Forest, Test set",
    x = "1 - Specificity",
    y = "Sensitivity",
    color = "Model"
  ) +
  theme(aspect.ratio = 1) # square, easier to tell what's going on

```

RF variable importance:

```{r}
rf_fit_prec |> 
  extract_fit_parsnip() |> 
  vip::vi() |> 
  slice(1:10) |> 
  ggplot(aes(x = Importance, y = forcats::fct_reorder(Variable, Importance), color = Variable)) +
  geom_segment(aes(xend = 0, yend = Variable), size = 2, alpha = 0.5) +
  geom_point(size = 4) +
  theme_minimal() +
  labs(
    title = "Variable Importance",
    subtitle = "Random Forest",
    x = "Importance (Impurity)",
    y = "Feature"
  ) +
  theme(
    legend.position = "none",
    axis.text.y = element_text(size = 12)
  )
```

# Bootstrapped confidence intervals

Given we cannot use `tune::int_pctl()` with the neural network, I am just doing both manually.

```{r}
keras3::set_random_seed(42)

test_boot <- 
  bootstraps(test, times = 500, strata = "spam") |> 
  # no splits, we only want to make predictions:
  mutate(data = map(splits, analysis)) |> 
  select(id, data)

nn_boot <- 
  test_boot |> 
  mutate(
    metrics_dnn = map(data, function(sample) {
      # The model is standalone, not a "workflow",
      # so we need to send the data through the prep pipeline
      # manually & then convert to matrix format
      X <- 
        spam_rec |> 
        prep() |> 
        bake(new_data = sample) |> 
        select(-spam) |> 
        as.matrix() |> 
        unname()
      
      mlp$predict(X, verbose = 0) |> 
        as.vector() |> 
        round() |> 
        tibble(mlp_pred = _) |> 
        bind_cols(sample) |> 
        mutate(
          mlp_pred = factor(
            if_else(mlp_pred == 1, "spam", "no spam"), levels = c("spam", "no spam")
          )
        ) |> 
        class_metrics(truth = spam, estimate = mlp_pred) |> 
        select(-.estimator) |> 
        pivot_wider(names_from = ".metric", values_from = ".estimate") |> 
        mutate(name = "Neural Network")
    })
  ) |> 
  unnest(metrics_dnn)

rf_boot <- 
  test_boot |> 
  mutate(
    metrics_rf = map(data, function(sample) {
      # This is a workflow object containing the preprocessing pipeline
      # and model that can just take any data directly:
      rf_fit_prec |> 
        augment(new_data = sample) |> 
        class_metrics(truth = spam, estimate = .pred_class) |> 
        select(-.estimator) |> 
        pivot_wider(names_from = ".metric", values_from = ".estimate") |> 
        mutate(name = "Random Forest")
    })
  ) |> 
  unnest(metrics_rf)
```

Evaluating:

```{r}
fns <- list(
  mean = mean,
  # 95% confidence intervals:
  lower = \(x) mean(x) - 1.96 * (sd(x) / sqrt(500)), # 500 = n
  upper = \(x) mean(x) + 1.96 * (sd(x) / sqrt(500))
)

boot_res <- 
  nn_boot |> 
  select(-c(id, data)) |> 
  bind_rows(rf_boot |> select(-c(id, data))) |> 
  pivot_longer(-name, names_to = "metric", values_to = "estimate") |> 
  group_by(name, metric) |> 
  summarise(across(estimate, fns, .names = "{fn}")) |> 
  ungroup()

boot_res
```

Inspecting graphically:

```{r}
boot_res |> 
  mutate(metric = if_else(metric == "f_meas", "F1", metric) |> stringr::str_to_title()) |> 
  ggplot(aes(x = metric, y = mean, color = name)) +
  geom_point(position = position_dodge(0.1)) +
  geom_errorbar(aes(ymin = lower, ymax = upper), position = position_dodge(0.1), width = .05) +
  theme_minimal() +
  labs(
    title = "Performance on test set",
    subtitle = "Bootstrapped 95% confidence intervals",
    x = "Metric",
    y = "Estimate",
    color = ""
  )
```

