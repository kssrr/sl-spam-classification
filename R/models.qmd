---
title: "Building neural network & competitor"
editor: source
date: today
format: 
  html:
    embed-resources: true
    message: false
    warning: false
    highlight-style: pygments
editor_options: 
  chunk_output_type: console
---

<style>
@import url('https://fonts.googleapis.com/css2?family=Inconsolata:wght@200..900&display=swap');

pre, code {
  font-family: 'Inconsolata', monospace;
  font-size: 18px !important;
}
</style>

# Packages

```{r}
library(discrim)
library(keras3)
library(tensorflow)
library(tidymodels)
```

# Data

```{r}
set.seed(42)

spam <- readr::read_csv(here::here("data/spam.csv"))

spam <- 
  spam |> 
  mutate(
    spam = factor(
      if_else(spam == 0, "no spam", "spam"),
      ordered = TRUE,
      levels = c("spam", "no spam")
    )
  )

spam_split <- initial_validation_split(spam, prop = c(0.6, 0.2), strata = "spam")

train <- training(spam_split)
val <- validation(spam_split)
test <- testing(spam_split)
```

# Preprocessing

Fitting the recipe to the training data (to avoid leakage). Synthetic minority class oversampling, to get the same amount of examples for both classes, log-transforming[^log] Z-standardizing, dropping highly correlated features & those with near-zero variance:

[^log]: Most of these features are word counts of some sort, likely with very skewed distributions.

```{r}
spam_rec <- 
  recipe(spam ~ ., data = train) |> 
  themis::step_smote(spam, over_ratio = 1, neighbors = 5) |> 
  step_log(all_numeric_predictors(), offset = 1) |> 
  step_range(all_numeric_predictors(), min = 0, max = 1) |> 
  step_corr(all_numeric_predictors(), threshold = 0.9) |> 
  step_nzv(all_numeric_predictors())
```

# EDA

Class distribution:

```{r}
train |> 
  count(spam) |> 
  ggplot(aes(x = spam, y = n, color = spam, fill = spam)) +
  geom_col(alpha = 0.7) +
  theme_minimal() +
  labs(
    title = "Class Distribution",
    x = "", 
    y = "N. of obs"
  ) +
  theme(legend.position = "none")
```

Feature correlations:

```{r}
corrs <- 
  train |> 
  select(-spam) |> 
  cor() |> 
  as.data.frame() |> 
  rownames_to_column(var = "x1") |> 
  tibble() |> 
  pivot_longer(-x1, names_to = "x2", values_to = "val")

corrs |> 
  ggplot(aes(x = x1, y = x2, fill = val)) +
  geom_tile() +
  scale_fill_distiller(palette = "RdYlGn", direction = 1, limits = c(-1, 1)) +
  theme(axis.text = element_blank(), axis.ticks = element_blank()) +
  labs(
    title = "Pairwise correlations", 
    subtitle = "All features", 
    fill = "Pearson's r", 
    x = "",
    y = ""
  )
```

PCA: checking if we can find some patterns in a transformed representation (also seeing if PCA as part of the preprocessing pipeline might make sense)

```{r}
spam_rec |> 
  step_pca(all_numeric_predictors(), num_comp = 4) |> 
  prep() |> 
  bake(new_data = train) |> 
  ggplot(aes(x = .panel_x, y = .panel_y, color = spam, fill = spam)) +
  geom_point(alpha = 0.4, size = 0.5) +
  ggforce::geom_autodensity(alpha = .3) +
  ggforce::facet_matrix(vars(-spam), layer.diag = 2) + 
  scale_color_brewer(palette = "Dark2", direction = -1) + 
  scale_fill_brewer(palette = "Dark2", direction = -1) +
  theme_minimal() +
  labs(title = "Principal Component Analysis", fill = "", color = "")
```

# Neural Network Classifier

Preparing the data (separate features & labels, bring into matrix format). We also need to apply the fitted preprocessing pipeline here for the data going into keras. `prep()` fits the recipe (on the training data, we specified this in the recipe), and `bake()` applies the transformation (equivalent to `.fit()` and `.transform()` in sklearn pipelines):

```{r}
keras_split <- function(set) {
  df <- 
    set |> 
    mutate(spam = if_else(spam == "spam", 1, 0))
  
  list(
    X = df |> select(-spam) |> as.matrix() |> unname(),
    y = df |> pull(spam) |> as.matrix()
  )
}

keras_train <- spam_rec |> prep() |> bake(new_data = train) |> keras_split()
keras_val <-  spam_rec |> prep() |> bake(new_data = val) |> keras_split()
keras_test <- spam_rec |> prep() |> bake(new_data = test) |> keras_split()

X_train <- keras_train$X
y_train <- keras_train$y
X_val <- keras_val$X
y_val <- keras_val$y
X_test <- keras_test$X
y_test <- keras_test$y
```

**Model**:

```{r}
set.seed(42)

mlp <- keras_model_sequential(
  layers = list(
    layer_dense(units = 16, activation = "relu", kernel_regularizer = regularizer_l2(0.01)),
    layer_dropout(rate = 0.25),
    layer_dense(units = 32, activation = "relu", kernel_regularizer = regularizer_l2(0.01)),
    layer_dropout(rate = 0.25),
    layer_dense(units = 16, activation = "relu", kernel_regularizer = regularizer_l2(0.01)),
    layer_dense(units = 1, activation = "sigmoid")
  )
)
```

Compiling (ignore cuda errors):

```{r}
set.seed(42)

mlp |> 
  compile(
    optimizer = optimizer_adam(learning_rate = 0.001),
    loss = "binary_crossentropy",
    metrics = list(
      metric_binary_accuracy(),
      metric_precision(),
      metric_recall(),
      metric_auc()
    )
  )
```

**Training**:

```{r}
set.seed(42)

history <- 
  mlp |> 
  fit(
    x = X_train,
    y = y_train,
    epochs = 100L,
    batch_size = 32L,
    validation_data = list(X_val, y_val),
    callbacks = list(
      # early stopping:
      callback_early_stopping(
        monitor = "val_loss",
        patience = 5L,
        restore_best_weights = TRUE 
      ),
      # schedule learning rate:
      callback_reduce_lr_on_plateau(
        monitor = "val_loss",
        factor = 0.5,
        patience = 3L,
        min_lr = 0.00001
      )
    )
  )
```

Looking at the training progression:

```{r}
training_prog <- 
  history |> 
  as.data.frame() |> 
  tibble() |>
  pivot_wider(values_from = "value", names_from = "metric") |> 
  drop_na(loss)
```

Loss curves:

```{r}
training_prog |> 
  ggplot(aes(x = epoch, y = loss, color = data)) +
  geom_line() +
  theme_minimal() +
  labs(
    title = "Training curves",
    subtitle = "Binary cross-entropy loss on training and validation sets, over epochs",
    x = "Epochs",
    y = "Loss",
    color = "Data"
  )
```

Validation metrics:

```{r}
training_prog |> 
  select(-c(learning_rate, loss)) |> 
  pivot_longer(-c(epoch, data), names_to = "metric", values_to = "value") |> 
  ggplot(aes(x = epoch, y = value, color = data)) +
  geom_line() +
  facet_wrap(~metric) +
  theme_minimal() +
  labs(
    title = "Training improvements",
    subtitle = "Development of metrics over epochs, validation set",
    x = "Epochs",
    y = "",
    color = "Data"
  )
```

Collecting final metrics for training set:

```{r}
class_metrics <- metric_set(accuracy, precision, recall)

mlp_metrics_train <- 
  mlp$predict(X_train) |> 
  round() |> 
  as.vector() |> 
  tibble(mlp_pred = _) |> 
  bind_cols(train) |> 
  mutate(mlp_pred = factor(if_else(mlp_pred == 1, "spam", "no spam"), levels = c("spam", "no spam"))) |> 
  class_metrics(truth = spam, estimate = mlp_pred) |> 
  select(-.estimator) |> 
  pivot_wider(names_from = ".metric", values_from = ".estimate") |> 
  mutate(name = "Neural Network")
```


Evaluating the model on the test set:

```{r}
mlp_preds <- 
  mlp$predict(X_test) |> 
  round() |> 
  as.vector() |> 
  tibble(mlp_pred = _) |> 
  bind_cols(test) |> 
  mutate(mlp_pred = factor(if_else(mlp_pred == 1, "spam", "no spam"), levels = c("spam", "no spam")))
```

Metrics:

```{r}
mlp_preds |> 
  class_metrics(truth = spam, estimate = mlp_pred)
```

Confusion matrix:

```{r}
mlp_preds |> 
  conf_mat(truth = spam, estimate = mlp_pred) |> 
  autoplot(type = "heatmap")
```

# Competitors

10-fold cross validation:

```{r}
set.seed(42)

folds <- vfold_cv(train, v = 10, strata = "spam")
```

To select a proper competitor, I tune three "traditional" models:

* Penalized logistic regression
* Random Forest
* Naive Bayes

```{r}
set.seed(42)

log_spec <- logistic_reg(
  mode = "classification",
  engine = "glmnet",
  penalty = tune(),
  mixture = 1 # pure L1 regularization
)

log_grid <- tibble(penalty = 10^seq(-5, -1, length.out = 50))

rf_spec <- rand_forest(
  mode = "classification",
  mtry = tune(),
  trees = tune(),
  min_n = tune()
) |> set_engine("ranger", importance = "impurity") # variable importance

rf_grid <- expand_grid(
  mtry = c(4, 8, 12),
  trees = c(100, 500, 1000),
  min_n = c(5, 10, 20)
)

# For naive bayes, we disable kernel density estimation. This yields
# Gaussian naive bayes, so we assume normally distributed features
# (seems reasonable given our preprocessing)
nb_spec <- naive_Bayes(
  mode = "classification",
  smoothness = tune()
) |> set_engine("naivebayes", usekernel = FALSE)

# just using raw probabilities (smoothing has no effect for Gaussian bayes
# with only numerical features I believe), but also cross-validating
nb_grid <- tibble(smoothness = 0)
```

Glueing together & tuning:

```{r}
set.seed(42)

models <- tribble(
  ~name,                 ~spec,    ~grid,
  "Logistic Regression", log_spec, log_grid,
  "Random Forest",       rf_spec,  rf_grid,
  "Naive Bayes",         nb_spec,  nb_grid
)

models <- 
  models |> 
  mutate(
    workflow = map(spec, \(m) workflow() |> add_recipe(spam_rec) |> add_model(m)),
    tuning_res = map2(workflow, grid, function(wf, g) {
      tune_grid(
        wf, 
        resamples = folds, # switch to `resamples` for bootstrapping instead
        grid = g,
        metrics = metric_set(accuracy, precision, recall, roc_auc),
        control = control_grid(verbose = TRUE, save_pred = TRUE)
      )
    }),
    metrics = map(tuning_res, collect_metrics)
  )
```

## Tuning results

First, adding 95% confidence intervals to metric estimates (estimated using the results collected across folds):

```{r}
models <- 
  models |> 
  mutate(
    metrics = map(metrics, function(m) { 
      m |> mutate(lower = mean - 1.96 * std_err, upper = mean + 1.96 * std_err)
    })
  )
```

### Logistic Regression

Here, we tuned the `penalty` parameter:

```{r}
models |> 
  filter(name == "Logistic Regression") |> 
  dplyr::select(name, metrics) |> 
  unnest(metrics) |> 
  ggplot(aes(x = penalty, y = mean, fill = .metric)) +
  geom_ribbon(aes(ymin = lower, ymax = upper), alpha = 0.2) +
  geom_point(aes(color = .metric)) +
  geom_line(aes(color = .metric)) +
  facet_wrap(~.metric, scales = "free_y") +
  scale_x_log10() +
  theme_minimal() +
  theme(legend.position = "none") +
  labs(
    title = "Logistic Regression", 
    subtitle = "Tuning Results", 
    x = expression(lambda),
    y = "",
    caption = "Note: x-axis is logarithmic"
  )
```

### Random Forest

```{r}
models |> 
  filter(name == "Random Forest") |> 
  dplyr::select(name, metrics) |> 
  unnest(metrics) |> 
  mutate(min_n = factor(min_n, ordered = TRUE)) |> 
  ggplot(aes(x = mtry, y = mean, group = min_n)) +
  geom_ribbon(aes(ymin = lower, ymax = upper, fill = min_n), alpha = .1) +
  geom_point(aes(color = min_n, )) +
  geom_line(aes(color = min_n, )) +
  facet_grid(vars(.metric), vars(trees), scale = "free_y") +
  theme_minimal() +
  labs(
    title = "Random Forest",
    subtitle = "Tuning Results, by number of trees",
    x = "Number of randomly sampled features",
    y = "",
    fill = "Min. N in \nnode for split",
    color = "Min. N in \nnode for split"
  )
```

### Naive Bayes

```{r}
models |> 
  filter(name == "Naive Bayes") |> 
  dplyr::select(name, metrics) |> 
  unnest(metrics) |> 
  ggplot(aes(x = smoothness, y = mean, color = .metric)) +
  geom_point() +
  geom_linerange(aes(ymin = lower, ymax = upper)) +
  facet_wrap(~.metric, nrow = 1) +
  theme_minimal() +
  theme(legend.position = "none", axis.text.x = element_blank()) +
  labs(
    title = "Naive Bayes",
    subtitle = "Metric estimates across 10 folds",
    x = "",
    y = ""
  )
```

## Uncertainty estimation for best competitors

First, we are interested in accuracy only. Using bootstrapping to estimate uncertainty of the accuracy estimate. Each model is evaluated on 50 stratified resamples of the train set:

```{r}
set.seed(42)

best_models_acc <- 
  models |> 
  mutate(
    best_params = map(tuning_res, \(res) select_best(res, metric = "accuracy")),
    model_best = map2(spec, best_params, \(mod, params) finalize_model(mod, params))
  ) |> 
  select(name, model_best) |> 
  deframe()

resamples <- bootstraps(train, times = 50, strata = "spam")

models_acc_wf <- workflow_set(
  preproc = list("General Prep" = spam_rec),
  models = best_models_acc
)

bs_results_acc <- workflow_map(
  models_acc_wf,
  resamples = resamples, 
  verbose = TRUE,
  control = control_resamples(save_pred = TRUE, verbose = TRUE),
  metrics = metric_set(accuracy)
)
```

Investigating:

```{r}
bs_results_acc |> 
  collect_metrics() |> 
  mutate(
    lower = mean - 1.96 * std_err,
    upper = mean + 1.96 * std_err,
    model = model |> gsub("_", " ", x  = _) |> stringr::str_to_title()
  ) |> 
  ggplot(aes(x = model, y = mean, color = model)) +
  geom_hline(yintercept = mlp_metrics_train$accuracy, lty = "dashed", color = "grey") +
  geom_point() +
  geom_linerange(aes(ymin = lower, ymax = upper)) +
  theme_minimal() +
  labs(
    title = "Uncertainty of accuracy estimates",
    subtitle = "Competitor models, training data",
    x = "",
    y = "Accuracy"
  ) +
  theme(legend.position = "none") +
  annotate(
    "text", 
    x = 0.67, 
    y = mlp_metrics_train$accuracy + 0.002, 
    label = "Neural Network",
    color = "grey50",
    size = 4
  )
```

Given we are classifying emails as spam, false positives (classifying "ham" emails as spam) are more costly, so we might want to optimize for that. Also estimating uncertainty:

```{r}
best_models_prec <- 
  models |> 
  mutate(
    best_params = map(tuning_res, \(res) select_best(res, metric = "precision")),
    model_best = map2(spec, best_params, \(mod, params) finalize_model(mod, params))
  ) |> 
  select(name, model_best) |> 
  deframe()

models_prec_wf <- workflow_set(
  preproc = list("General Prep" = spam_rec),
  models = best_models_prec
)

bs_results_prec <- workflow_map(
  models_prec_wf,
  resamples = resamples, 
  verbose = TRUE,
  control = control_resamples(save_pred = TRUE, verbose = TRUE),
  metrics = metric_set(accuracy)
)
```

Inspecting:

```{r}
bs_results_prec |> 
  collect_metrics() |> 
  mutate(
    lower = mean - 1.96 * std_err,
    upper = mean + 1.96 * std_err,
    model = model |> gsub("_", " ", x  = _) |> stringr::str_to_title()
  ) |> 
  ggplot(aes(x = model, y = mean, color = model)) +
  geom_hline(yintercept = mlp_metrics_train$precision, lty = "dashed", color = "grey") +
  geom_point() +
  geom_linerange(aes(ymin = lower, ymax = upper)) +
  theme_minimal() +
  labs(
    title = "Uncertainty of precision estimates",
    subtitle = "Competitor models, training data",
    x = "",
    y = "Precision"
  ) +
  theme(legend.position = "none") +
  annotate(
    "text", 
    x = 0.67, 
    y = mlp_metrics_train$precision + 0.002, 
    label = "Neural Network",
    color = "grey50",
    size = 4
  )
```

# Evaluating the best models on the test set

```{r}
test_eval <- 
  models |> 
  mutate(
    # best models by accuracy & precision:
    best_acc = map(tuning_res, \(res) select_best(res, metric = "accuracy")),
    best_prec = map(tuning_res, \(res) select_best(res, metric = "precision")),
    # fitting the models on the training set:
    across(c(best_acc, best_prec), function(params) {
      map2(workflow, params, function(wf, p) {
        wf |> 
          finalize_workflow(p) |> 
          fit(train)
      })
    }),
    # collecting predictions & metrics:
    across(c(best_acc, best_prec), function(model) {
      map(model, function(m) {
        m |> 
          augment(new_data = test) |> 
          class_metrics(truth = spam, estimate = .pred_class) |> 
          select(-.estimator) |> 
          pivot_wider(names_from = ".metric", values_from = ".estimate")
      })
    })
  )
```

Neural network (we already collected preds further above after being done with training):

```{r}
mlp_metrics_test <- 
  mlp_preds |> 
  class_metrics(truth = spam, estimate = mlp_pred) |> 
  select(-.estimator) |> 
  pivot_wider(names_from = ".metric", values_from = ".estimate") |> 
  mutate(name = "Neural Network")
```


Best by accuracy:

```{r}
# Some markdown magic:
mark_best <- function(x) {
  map_chr(x, function(val) {
    if (val == max(x))
      return(paste0("**", as.character(round(val, 3)), "**"))
    as.character(round(val, 3))
  })
}

test_eval |> 
  select(name, best_acc) |> 
  unnest(best_acc) |> 
  bind_rows(mlp_metrics_test) |> 
  mutate(across(-name, mark_best)) |> 
  knitr::kable()
```

Best by precision:

```{r}
test_eval |> 
  select(name, best_prec) |> 
  unnest(best_prec) |> 
  bind_rows(mlp_metrics_test) |> 
  select(name, precision) |> 
  mutate(precision = mark_best(precision)) |> 
  knitr::kable()
```

