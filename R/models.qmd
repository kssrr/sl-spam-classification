---
title: "Building neural network & competitor"
editor: source
date: today
format: 
  html:
    embed-resources: true
    message: false
    warning: false
    highlight-style: pygments
editor_options: 
  chunk_output_type: console
---

<style>
@import url('https://fonts.googleapis.com/css2?family=Inconsolata:wght@200..900&display=swap');

pre, code {
  font-family: 'Inconsolata', monospace;
  font-size: 18px !important;
}
</style>

# Packages

```{r}
library(discrim)
library(keras3)
library(tensorflow)
library(tidymodels)
```

# Data

General prep

```{r}
set.seed(42)

spam <- readr::read_csv(here::here("data/spam.csv"))

spam <- 
  spam |> 
  mutate(
    # outcome has to be ordered factor for tidymodels:
    spam = factor(
      if_else(spam == 0, "no spam", "spam"),
      ordered = TRUE,
      levels = c("spam", "no spam")
    )
  )

# Data split (60/20/20):
spam_split <- initial_validation_split(spam, prop = c(0.6, 0.2), strata = "spam")

train <- training(spam_split)
val <- validation(spam_split)
test <- testing(spam_split)
```

Getting an overview:

```{r}
glimpse(train)
```

A lot of these seem to be word or character frequencies, so I suspect that they might be sparse (a lot of zero values) and have skewed distributions. Investigating:

```{r}
#| fig-width: 12
#| fig-height: 12
train |> 
  select(-spam) |> 
  pivot_longer(cols = everything(), names_to = "Feature", values_to = "Value") |> 
  ggplot(aes(x = Value)) +
  geom_histogram(bins = 20, fill = "steelblue", color = "white") +
  facet_wrap(~ Feature, scales = "free") + 
  theme_minimal()
```

# Preprocessing

Below is the preprocessing [recipe](https://recipes.tidymodels.org/) I used. Synthetic minority class oversampling, to get the same amount of examples for both classes (although class imbalance is not too terrible in this case), log-transforming (mitigate some of the right skew), normalizing, dropping highly correlated features & those with near-zero variance:

```{r}
spam_rec <- 
  recipe(spam ~ ., data = train) |> 
  themis::step_smote(spam, over_ratio = 1, neighbors = 5) |> 
  step_log(all_numeric_predictors(), offset = 1) |> 
  step_range(all_numeric_predictors(), min = 0, max = 1) |> 
  step_corr(all_numeric_predictors(), threshold = 0.9) |> 
  step_nzv(all_numeric_predictors())
```

The recipe is specified to be fitted ("prepped") on the training data to avoid leakage.

# EDA

Class distribution:

```{r}
train |> 
  count(spam) |> 
  ggplot(aes(x = spam, y = n, color = spam, fill = spam)) +
  geom_col(alpha = 0.7) +
  theme_minimal() +
  scale_color_brewer(palette = "Dark2", direction = -1) + 
  scale_fill_brewer(palette = "Dark2", direction = -1) +
  labs(
    title = "Class Distribution",
    x = "", 
    y = "N. of obs"
  ) +
  theme(legend.position = "none")
```

Feature correlations (unlabelled, hard to see with this many features anyways, just to check *whether* highly correlated features exist):

```{r}
corrs <- 
  train |> 
  select(-spam) |> 
  cor() |> 
  as.data.frame() |> 
  rownames_to_column(var = "x1") |> 
  tibble() |> 
  pivot_longer(-x1, names_to = "x2", values_to = "val")

corrs |> 
  ggplot(aes(x = x1, y = x2, fill = val)) +
  geom_tile() +
  scale_fill_distiller(palette = "RdYlGn", direction = 1, limits = c(-1, 1)) +
  theme(axis.text = element_blank(), axis.ticks = element_blank()) +
  labs(
    title = "Pairwise correlations", 
    subtitle = "All features", 
    fill = "Pearson's r", 
    x = "",
    y = ""
  )
```

PCA: checking if we can find some patterns in a transformed representation (also seeing if PCA as part of the preprocessing pipeline might make sense)[^pca]:

```{r}
spam_rec |> 
  step_pca(all_numeric_predictors(), num_comp = 4) |> 
  prep() |> 
  bake(new_data = train) |> 
  ggplot(aes(x = .panel_x, y = .panel_y, color = spam, fill = spam)) +
  geom_point(alpha = 0.25, size = 0.5) +
  ggforce::geom_autodensity(alpha = .3) +
  ggforce::facet_matrix(vars(-spam), layer.diag = 2) + 
  scale_color_brewer(palette = "Dark2", direction = -1) + 
  scale_fill_brewer(palette = "Dark2", direction = -1) +
  theme_minimal() +
  labs(title = "Principal Component Analysis", fill = "", color = "")
```

[^pca]: I tried this (it was tempting to just magically generate richt & uncorrelated features instead of dealing with the existing ones, but that would make the neural network more unstable)

# Neural Network Classifier

Preparing the data (separate features & labels, bring into matrix format). We also need to apply the fitted preprocessing pipeline here for the data going into keras. `prep()` fits the recipe (on the training data, we specified this in the recipe), and `bake()` applies the transformation (equivalent to `.fit()` and `.transform()` in sklearn pipelines):

```{r}
keras_split <- function(set) {
  df <- 
    set |> 
    mutate(spam = if_else(spam == "spam", 1, 0))
  
  list(
    X = df |> select(-spam) |> as.matrix() |> unname(),
    y = df |> pull(spam) |> as.matrix()
  )
}

keras_train <- spam_rec |> prep() |> bake(new_data = train) |> keras_split()
keras_val <-  spam_rec |> prep() |> bake(new_data = val) |> keras_split()
keras_test <- spam_rec |> prep() |> bake(new_data = test) |> keras_split()

X_train <- keras_train$X
y_train <- keras_train$y
X_val <- keras_val$X
y_val <- keras_val$y
X_test <- keras_test$X
y_test <- keras_test$y
```

**Model**:

```{r}
keras3::set_random_seed(42) 
#^ the keras3 version (supposedly) sets a seed for the R session and the whole backend

mlp <- keras_model_sequential(
  layers = list(
    layer_dense(units = 128, activation = "relu", kernel_regularizer = regularizer_l2(0.001)),
    layer_dropout(rate = 0.25),
    layer_dense(units = 64, activation = "relu", kernel_regularizer = regularizer_l2(0.001)),
    layer_dropout(rate = 0.25),
    layer_dense(units = 32, activation = "relu", kernel_regularizer = regularizer_l2(0.001)),
    layer_dropout(rate = 0.25),
    layer_dense(units = 16, activation = "relu", kernel_regularizer = regularizer_l2(0.001)),
    layer_dropout(rate = 0.25),
    layer_dense(units = 1, activation = "sigmoid")
  )
)
```

Compiling:

```{r}
keras3::set_random_seed(42)
# chunks are independent when rendering apparently, so again...

mlp |> 
  compile(
    optimizer = optimizer_adam(learning_rate = 0.001),
    loss = "binary_crossentropy",
    metrics = list(
      metric_binary_accuracy(name = "Accuracy"),
      metric_precision(name = "Precision"),
      metric_recall(name = "Recall"),
      metric_f1_score(average = "micro", threshold = .5, name = "F1")
    )
  )
```

**Training**:

```{r}
keras3::set_random_seed(42)

history <- 
  mlp |> 
  fit(
    x = X_train,
    y = y_train,
    epochs = 250L,
    batch_size = 32L,
    validation_data = list(X_val, y_val),
    callbacks = list(
      # early stopping:
      callback_early_stopping(
        monitor = "val_loss",
        patience = 5L,
        restore_best_weights = TRUE 
      ),
      # schedule learning rate:
      callback_reduce_lr_on_plateau(
        monitor = "val_loss",
        factor = 0.8,
        patience = 3L,
        min_lr = 0.00001
      )
    ),
    shuffle = FALSE
  )
```

Details on the trained model:

```{r}
summary(mlp)
```

Visually:

```{r}
mlp |> plot(show_shapes = TRUE, show_trainable = TRUE)
```

Looking at the training progression:

```{r}
training_prog <- 
  history |> 
  as.data.frame() |> 
  tibble() |>
  pivot_wider(values_from = "value", names_from = "metric") |> 
  drop_na(loss)
```

Loss curves:

```{r}
training_prog |> 
  ggplot(aes(x = epoch, y = loss, color = data)) +
  geom_line() +
  theme_minimal() +
  labs(
    title = "Training curves",
    subtitle = "Binary cross-entropy loss on training and validation sets, over epochs",
    x = "Epochs",
    y = "Loss",
    color = "Data"
  )
```

Validation metrics:

```{r}
training_prog |> 
  select(-c(learning_rate, loss)) |> 
  pivot_longer(-c(epoch, data), names_to = "metric", values_to = "value") |> 
  ggplot(aes(x = epoch, y = value, color = data)) +
  geom_line() +
  facet_wrap(~metric) +
  theme_minimal() +
  labs(
    title = "Training improvements",
    subtitle = "Development of metrics over epochs, validation set",
    x = "Epochs",
    y = "",
    color = "Data"
  )
```

Collecting final metrics for training set:

```{r}
class_metrics <- metric_set(accuracy, precision, recall, f_meas)

mlp_metrics_train <- 
  mlp$predict(X_train) |> 
  round() |> 
  as.vector() |> 
  tibble(mlp_pred = _) |> 
  bind_cols(train) |> 
  mutate(mlp_pred = factor(if_else(mlp_pred == 1, "spam", "no spam"), levels = c("spam", "no spam"))) |> 
  class_metrics(truth = spam, estimate = mlp_pred) |> 
  select(-.estimator) |> 
  pivot_wider(names_from = ".metric", values_from = ".estimate") |> 
  mutate(name = "Neural Network")
```

Evaluating the model on the test set:

```{r}
mlp_preds <- 
  mlp$predict(X_test) |> 
  round() |> 
  as.vector() |> 
  tibble(mlp_pred = _) |> 
  bind_cols(test) |> 
  mutate(mlp_pred = factor(if_else(mlp_pred == 1, "spam", "no spam"), levels = c("spam", "no spam")))
```

Metrics:

```{r}
mlp_preds |> 
  class_metrics(truth = spam, estimate = mlp_pred)
```

Confusion matrix:

```{r}
mlp_preds |> 
  conf_mat(truth = spam, estimate = mlp_pred) |> 
  autoplot(type = "heatmap")
```

# Competitors

10-fold cross validation:

```{r}
set.seed(42)

folds <- vfold_cv(train, v = 10, strata = "spam")
```

To select a proper competitor, I tune three "traditional" models:

* Penalized logistic regression
* Random Forest
* Naive Bayes

Model specifications:

```{r}
set.seed(42)

log_spec <- logistic_reg(
  mode = "classification",
  engine = "glmnet",
  penalty = tune(),
  mixture = 1 # pure L1 regularization
)

rf_spec <- rand_forest(
  mode = "classification",
  mtry = tune(),
  trees = tune(),
  min_n = tune()
) |> set_engine("ranger", importance = "impurity") # variable importance

# For naive bayes, we disable kernel density estimation. This yields
# Gaussian naive bayes, so we assume normally distributed features
nb_spec <- naive_Bayes(
  mode = "classification",
  smoothness = tune()
) |> set_engine("naivebayes", usekernel = FALSE)
```

Hyperparameter grids (I do regular grid search, given the number of hyperparameters to tune is fairly small; the maximum is three for the random forest, so imo strategies like bayesian tuning don't really pay off):

```{r}
log_grid <- tibble(penalty = 10^seq(-5, -1, length.out = 50))

rf_grid <- expand_grid(
  mtry = c(4, 8, 12),
  trees = c(250, 500, 1000),
  min_n = c(5, 10, 20)
)

# just using raw probabilities (smoothing has no effect for Gaussian bayes
# with only numerical features I believe), but also cross-validating
nb_grid <- tibble(smoothness = 0)
```

Tidymodels does have `workflow_set` and `workflow_map` for multiple models, but I found them awkward to work with, so just glueing this together myself:

```{r}
set.seed(42)

competitors <- tribble(
  ~name,                 ~spec,    ~grid,
  "Logistic Regression", log_spec, log_grid,
  "Random Forest",       rf_spec,  rf_grid,
  "Naive Bayes",         nb_spec,  nb_grid
)

competitors <- 
  competitors |> 
  mutate(
    # bundle recipe & models into workflows:
    workflow = map(spec, \(m) workflow() |> add_recipe(spam_rec) |> add_model(m)),
    # running grid search for all models:
    tuning_res = map2(workflow, grid, function(wf, g) {
      tune_grid(
        wf, 
        resamples = folds,
        grid = g,
        metrics = class_metrics,
        control = control_grid(verbose = TRUE, save_pred = TRUE)
      )
    }),
    metrics = map(tuning_res, collect_metrics)
  )
```

The `workflow`-objects are now like a "pipeline" consisting of the preprocessing recipe and then the model (see [here](https://workflows.tidymodels.org/)).

Looking at the tuning results for the competitor models. I consider precision as the most important metric - false positives (i.e. falsely labeling "ham" emails as spam) are more costly:

```{r}
competitors |> 
  select(name, metrics) |> 
  unnest(metrics) |> 
  filter(.metric == "precision") |> 
  arrange(desc(mean)) |> 
  mutate(
    # 95% confidence intervals:
    lower = mean - 1.96 * std_err,
    upper = mean + 1.96 * std_err
  ) |> 
  mutate(rank = row_number()) |> 
  ggplot(aes(x = rank, y = mean, color = name)) +
  geom_point() +
  geom_errorbar(aes(ymin = lower, ymax = upper), width = 0.5) +
  geom_hline(yintercept = mlp_metrics_train$accuracy, lty = "dashed", color = "grey") +
  annotate(
    "text", 
    x = 70, 
    y = mlp_metrics_train$accuracy + 0.003, 
    label = "Neural Network",
    color = "grey50",
    size = 4
  ) +
  theme_minimal() +
  labs(
    title = "Competitors: Tuning Results",
    subtitle = "Precision, estimated across 10 folds",
    x = "Model Rank",
    y = "Precision",
    color = "Model",
    caption = "Bars indicate 95% confidence intervals"
  ) +
  theme(legend.position = "bottom")
```

Looks like the random forest has the best chances of performing on par with - or even outperforming - the neural network. Pulling the best hyperparameters:

```{r}
best_rf_params <- 
  competitors |> 
  filter(name == "Random Forest") |> 
  pull(tuning_res) |> 
  first() |> 
  select_best(metric = "precision")

best_rf_params
```

Fitting the best model to the training set:

```{r}
rf_fit <- 
  competitors |> 
  filter(name == "Random Forest") |> 
  pull(workflow) |> 
  first() |> 
  finalize_workflow(best_rf_params) |> 
  fit(train)
```

## Final Random forest vs. NN on the train set

Collecting train preds & metrics and comparing to neural network:

```{r}
# Some markdown magic:
mark_best <- function(x) {
  map_chr(x, function(val) {
    if (val == max(x))
      return(paste0("**", as.character(round(val, 3)), "**"))
    as.character(round(val, 3))
  })
}

rf_fit |> 
  augment(new_data = train) |> 
  class_metrics(truth = spam, estimate = .pred_class) |> 
  pivot_wider(names_from = ".metric", values_from = ".estimate") |> 
  select(-.estimator) |> 
  mutate(name = "Random Forest") |> 
  bind_rows(mlp_metrics_train) |> 
  select(name, precision, recall, f_meas, accuracy) |> 
  mutate(across(-name, mark_best)) |> 
  arrange(name) |> 
  rename(f1 = f_meas) |> 
  rename_with(stringr::str_to_title) |> 
  knitr::kable()
```

At first when I saw those metrics I looked a little like this

![](../figures/me_when_99.jpg){width="50%"}

This looks like some overfitting is going on, but given that the model does not seem overly complex (e.g. chose 500 over 1000 trees, low number of randomly sampled predictors, both of which should work against model complexity and overfitting), that it was validated across 10 folds with performance rivaling that of the neural network, and that it still delivers pretty good test performance (see below) I concluded that this was still fine.

# Random Forest vs. Neural Network on test set

Test predictions for both (plus predicted class probability):

```{r}
nn_preds <- 
  mlp |> 
  predict(X_test) |> 
  as.vector() |> 
  tibble(.pred_spam = _) |> 
  mutate(
    .pred_no_spam = 1 - .pred_spam, 
    .pred_class = round(.pred_spam),
    .pred_class = factor(
      if_else(.pred_class == 1, "spam", "no spam"),
      ordered = TRUE,
      levels = c("spam", "no spam")
    ),
    model = "Neural Network"
  ) |> 
  bind_cols(test |> select(actual = spam))

rf_preds <- 
  rf_fit |> 
  predict(test) |> 
  bind_cols(rf_fit |> predict(test, type = "prob")) |>
  rename(.pred_no_spam = `.pred_no spam`) |> 
  mutate(
    model = "Random Forest",
    .pred_class = factor(.pred_class, ordered = TRUE, levels = c("spam", "no spam"))
  ) |> 
  bind_cols(test |> select(actual = spam))

test_preds <- bind_rows(nn_preds, rf_preds)
```

Metrics:

```{r}
test_preds |> 
  group_by(model) |> 
  nest(-model) |> 
  mutate(
    metrics = map(data, \(preds) {
      preds |> 
        class_metrics(truth = actual, estimate = .pred_class) |> 
        select(-.estimator) |> 
        pivot_wider(names_from = ".metric", values_from = ".estimate")
    })
  ) |> 
  select(model, metrics) |> 
  unnest(metrics) |> 
  rename(f1 = f_meas) |> 
  ungroup() |> 
  select(model, precision, recall, f1, accuracy) |> 
  mutate(across(-model, mark_best)) |> 
  rename_with(stringr::str_to_title) |> 
  knitr::kable()
```

Looking at model confidence. Graphically, we can see that the neural network is more confident in its correct predictions, but also overconfident in its wrong predictions:

```{r}
confidence <- 
 test_preds |> 
  mutate(
    confidence = if_else(.pred_class == "spam", .pred_spam, .pred_no_spam),
    correct = if_else(actual == .pred_class, "correct", "incorrect")
  )

confidence |> 
  ggplot(aes(x = correct, y = confidence, fill = model, color = model)) +
  geom_hline(yintercept = c(.5, 1), lty = "dotted", color = "grey50") +
  geom_boxplot(position = position_dodge(width = 0.2), width = .1, outliers = FALSE, alpha = .5) +
  theme_minimal() +
  labs(
    title = "Confidence in predictions",
    subtitle = "By correct/incorrect classification",
    x = "",
    y = "Predicted class probability",
    fill = "Model",
    color = "Model"
  )
```

Other way of looking at it (making both & then deciding later for report):

```{r}
confidence |> 
  ggplot(aes(x = confidence, color = model, fill = model)) +
  geom_density(alpha = .34) +
  facet_wrap(~correct, nrow = 2, scale = "free_y") +
  theme_minimal()  +
  labs(
    title = "Confidence in predictions",
    subtitle = "By correct/incorrect classification",
    x = "Predicted class probability",
    y = "Density",
    fill = "Model",
    color = "Model"
  ) +
  theme(aspect.ratio = .5)
```

ROC curves:

```{r}
test_preds |> 
  group_by(model) |>
  roc_curve(truth = actual, .pred_spam) |>
  ungroup() |> 
  ggplot(aes(x = 1 - specificity, y = sensitivity, color = model)) +
  geom_line() + 
  geom_abline(linetype = "dotted", color = "grey50") +
  theme_minimal() +
  labs(
    title = "ROC Curves",
    subtitle = "Neural Network & Random Forest, Test set",
    x = "1 - Specificity",
    y = "Sensitivity",
    color = "Model"
  ) +
  theme(aspect.ratio = 1) # square, easier to tell what's going on

```

Confusion matrices:

```{r}
test_preds |> 
  group_by(model) |> 
  conf_mat(truth = actual, estimate = .pred_class) |> 
  mutate(plot = map2(model, conf_mat, function(n, c) {
    c |> 
      autoplot(type = "heatmap") + 
      labs(title = n)
  })) |> 
  pull(plot) |> 
  patchwork::wrap_plots(ncol = 2)
```

RF variable importance:

```{r}
rf_fit |> 
  extract_fit_parsnip() |> 
  vip::vi() |> 
  slice(1:10) |> 
  ggplot(aes(x = Importance, y = forcats::fct_reorder(Variable, Importance), color = Variable)) +
  geom_segment(aes(xend = 0, yend = Variable), size = 2, alpha = 0.5) +
  geom_point(size = 4) +
  theme_minimal() +
  labs(
    title = "Variable Importance",
    subtitle = "Random Forest",
    x = "Importance (Impurity)",
    y = "Feature"
  ) +
  theme(
    legend.position = "none",
    axis.text.y = element_text(size = 12)
  )
```

# Bootstrapped confidence intervals

Given we cannot use `tune::int_pctl()` with the neural network, I am just doing both manually.

```{r}
keras3::set_random_seed(42)

test_boot <- 
  bootstraps(test, times = 500, strata = "spam") |> 
  # no splits, we only want to make predictions:
  mutate(data = map(splits, analysis)) |> 
  select(id, data)

nn_boot <- 
  test_boot |> 
  mutate(
    metrics_dnn = map(data, function(sample) {
      # The model is standalone, not a "workflow",
      # so we need to send the data through the prep pipeline
      # manually & then convert to matrix format
      X <- 
        spam_rec |> 
        prep() |> 
        bake(new_data = sample) |> 
        select(-spam) |> 
        as.matrix() |> 
        unname()
      
      mlp$predict(X, verbose = 0) |> 
        as.vector() |> 
        round() |> 
        tibble(mlp_pred = _) |> 
        bind_cols(sample) |> 
        mutate(
          mlp_pred = factor(
            if_else(mlp_pred == 1, "spam", "no spam"), levels = c("spam", "no spam")
          )
        ) |> 
        class_metrics(truth = spam, estimate = mlp_pred) |> 
        select(-.estimator) |> 
        pivot_wider(names_from = ".metric", values_from = ".estimate") |> 
        mutate(name = "Neural Network")
    })
  ) |> 
  unnest(metrics_dnn)

rf_boot <- 
  test_boot |> 
  mutate(
    metrics_rf = map(data, function(sample) {
      # This is a workflow object containing the preprocessing pipeline
      # and model that can just take any data directly:
      rf_fit |> 
        augment(new_data = sample) |> 
        class_metrics(truth = spam, estimate = .pred_class) |> 
        select(-.estimator) |> 
        pivot_wider(names_from = ".metric", values_from = ".estimate") |> 
        mutate(name = "Random Forest")
    })
  ) |> 
  unnest(metrics_rf)
```

Evaluating:

```{r}
fns <- list(
  mean = mean,
  # 95% confidence intervals:
  lower = \(x) mean(x) - 1.96 * (sd(x) / sqrt(500)), # 500 = n
  upper = \(x) mean(x) + 1.96 * (sd(x) / sqrt(500))
)

boot_res <- 
  nn_boot |> 
  select(-c(id, data)) |> 
  bind_rows(rf_boot |> select(-c(id, data))) |> 
  pivot_longer(-name, names_to = "metric", values_to = "estimate") |> 
  group_by(name, metric) |> 
  summarise(across(estimate, fns, .names = "{fn}")) |> 
  ungroup()

boot_res
```

Inspecting graphically:

```{r}
boot_res |> 
  mutate(metric = if_else(metric == "f_meas", "F1", metric) |> stringr::str_to_title()) |> 
  ggplot(aes(x = metric, y = mean, color = name)) +
  geom_point(position = position_dodge(0.1)) +
  geom_errorbar(aes(ymin = lower, ymax = upper), position = position_dodge(0.1), width = .05) +
  theme_minimal() +
  labs(
    title = "Performance on test set",
    subtitle = "Bootstrapped 95% confidence intervals",
    x = "Metric",
    y = "Estimate",
    color = ""
  )
```

